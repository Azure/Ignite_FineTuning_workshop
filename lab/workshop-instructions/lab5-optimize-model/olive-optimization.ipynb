{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv6vx7wooDfk"
      },
      "source": [
        "# OLIVE: Model optimization toolkit for the ONNX Runtime\n",
        "\n",
        "OLIVE (ONNX LIVE) is a cutting-edge model optimization toolkit with accompanying CLI that enables you to ship models for the ONNX runtime with quality and performance.\n",
        "\n",
        "<img src=\"./images/olive-flow.png\" alt=\"Olive Flow\" width=\"500\"/>\n",
        "\n",
        "The input to OLIVE is typically a PyTorch or Hugging Face model and the output is an optimized ONNX model that is executed on a device (deployment target) running the ONNX runtime. OLIVE will optimize the model for the deployment target's AI accelerator (NPU, GPU, CPU) provided by a hardware vendor such as Qualcomm, AMD, Nvidia or Intel.\n",
        "\n",
        "OLIVE executes a workflow, which is an ordered sequence of individual model optimization tasks called passes - example passes include: model compression, graph capture, quantization, graph optimization. Each pass has a set of parameters that can be tuned to achieve the best metrics, say accuracy and latency, that are evaluated by the respective evaluator. OLIVE employs a search strategy that uses a search algorithm to auto-tune each pass one by one or set of passes together.\n",
        "\n",
        "## ‚ûï Benefits of OLIVE\n",
        "\n",
        "- Reduce frustration and time of trial-and-error manual experimentation with different techniquies for graph optimization, compression and quantization. Define your quality and performance constraints and let OLIVE automatically find the best model for you.\n",
        "- 40+ built-in model optimization components covering cutting edge techniques in quantization, compression, graph optimization and finetuning.\n",
        "- Easy-to-use CLI for common model optimization tasks. For example, olive quantize, olive auto-opt, olive finetune.\n",
        "- Model packaging and deployment built-in.\n",
        "- Supports Multi LoRA serving.\n",
        "- Construct workflows using YAML/JSON to orchestrate model optimization and deployment tasks.\n",
        "- Hugging Face and Azure AI Integration.\n",
        "- Built-in caching mechanism to save costs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTxn4ukNoDfo"
      },
      "source": [
        "## The data\n",
        "\n",
        "In this example, you're going to fine-tune Phi-3.5-Mini model so that it is specialized in answering travel related questions. The code below displays the first few records of the dataset, which are in JSON lines format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "deb0a1f3357b4e6090cce383a0368aab",
            "27bd650f5bf5433db248772ad5bfe3d9",
            "f5272b5ed355411db8baeb2b360cea5f",
            "5d8eaec8076a411198b291859ab1c9b6",
            "f42d39f701de48888145efa92da5fa02",
            "590d57c9b3f14250b48d3422b60992a0",
            "e4eaa3c7776343af9f54308b01db55f4",
            "9117830d46384e9a9c3c98c1b0d5bd8c",
            "3c87e2e2e92545809c6f8e6168dcc36c",
            "138535bf368b48ca9d5bc1f007f843ac",
            "e65680b8f29d491186d075b59c9e3d79",
            "344cf705b19b4549beed33873ccb2876",
            "fc93b5f2f32741a4af2a4f5a2c4a9e9c",
            "93d817aded5748ae93aee70d35c9d2a2",
            "45a0e8351ceb40339cd607d39d04d7a1",
            "e5cef26102904223a1432ac645db3dfe",
            "e3389975c0f443ad90d6f8cee4710577",
            "680edc52159147eabaee8d0ad171f879",
            "a4def9b16cd34f36bb4974757cce1319",
            "8aef653a8665451c878f32d13c0bf301",
            "d62297cd89d04114b76e57a3183fdfbd",
            "942a390409a648d8b980d5e7c62ee900",
            "b4b65045cd8c4474a9dd875d3b3d4e06",
            "077ebf9f5be54cc083697f77698e5ba6",
            "ee2aecdb35244cc0847a5f1363502018",
            "697c911fc7224cd88364293abe822d02",
            "4ae2eaf6c01e45038e77a2bf1ab22a4e",
            "3c18ad862d994fbfa7026b7b83925c2c",
            "091861fe24704925ab3f7a30bd1179d5",
            "7c261f5347b540e4b357348ff0e873c5",
            "12c5ab0e35424b7b97b64802f425a5b9",
            "dc0de3b5c3624715b4421ea6c1f858cf",
            "086dc4c982db41a0bc8c0131b79e50e7"
          ]
        },
        "id": "B_O8YbcSoDfp",
        "outputId": "82ab802f-428e-4718-de0a-2bb7e78e8c25"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What's a must-see in Paris?</td>\n",
              "      <td>Oh la la! You simply must twirl around the Eif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Best way to get around Tokyo?</td>\n",
              "      <td>Hop on a bullet train for speed, explore the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What's the best museum in New York?</td>\n",
              "      <td>The Met is a must-visit,t don't overlook the M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What should I pack for a trip to Australia?</td>\n",
              "      <td>Don't forget sunscreen and a hat for those sun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Best place to eat in Bangkok?</td>\n",
              "      <td>For street food heaven,y the night markets ‚Äì y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        prompt  \\\n",
              "0                  What's a must-see in Paris?   \n",
              "1                Best way to get around Tokyo?   \n",
              "2          What's the best museum in New York?   \n",
              "3  What should I pack for a trip to Australia?   \n",
              "4                Best place to eat in Bangkok?   \n",
              "\n",
              "                                            response  \n",
              "0  Oh la la! You simply must twirl around the Eif...  \n",
              "1  Hop on a bullet train for speed, explore the c...  \n",
              "2  The Met is a must-visit,t don't overlook the M...  \n",
              "3  Don't forget sunscreen and a hat for those sun...  \n",
              "4  For street food heaven,y the night markets ‚Äì y...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"data/data_sample_travel.jsonl\")\n",
        "dataset[\"train\"].to_pandas().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóúÔ∏è Quantize the model\n",
        "\n",
        "Before training the model, we first quantize it using a technique called [Active Aware Quantization (AWQ)](https://arxiv.org/abs/2306.00978) - below is the abstract of the AWQ paper.\n",
        "\n",
        "*Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.*\n",
        "\n",
        "We find that quantizing the model *before* fine-tuning greatly improves the accuracy of the model.\n",
        "\n",
        "> **üìù It takes around 10mins for the quantization to complete.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading HuggingFace model from microsoft/Phi-3.5-mini-instruct\n",
            "[2024-10-16 13:26:53,855] [INFO] [run.py:138:run_engine] Running workflow default_workflow\n",
            "[2024-10-16 13:26:53,869] [INFO] [cache.py:137:__init__] Using cache directory: /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/.olive-cache/default_workflow\n",
            "[2024-10-16 13:26:53,871] [INFO] [accelerator_creator.py:224:create_accelerators] Running workflow on accelerator specs: cpu-cpu\n",
            "[2024-10-16 13:26:53,871] [INFO] [engine.py:255:run] Running Olive on accelerator: cpu-cpu\n",
            "[2024-10-16 13:26:53,871] [INFO] [engine.py:897:_create_system] Creating target system ...\n",
            "[2024-10-16 13:26:53,872] [INFO] [engine.py:900:_create_system] Target system created in 0.000424 seconds\n",
            "[2024-10-16 13:26:53,872] [INFO] [engine.py:909:_create_system] Creating host system ...\n",
            "[2024-10-16 13:26:53,872] [INFO] [engine.py:912:_create_system] Host system created in 0.000042 seconds\n",
            "[2024-10-16 13:26:53,939] [INFO] [engine.py:718:_run_pass] Running pass awq:AutoAWQQuantizer {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 19 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:00<00:00, 200230.59it/s]\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.36it/s]\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "AWQ:   0%|          | 0/32 [00:00<?, ?it/s]WARNING:transformers_modules.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n",
            "AWQ: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [10:55<00:00, 20.48s/it]\n",
            "Note that `shard_checkpoint` is deprecated and will be removed in v4.44. We recommend you using split_torch_state_dict_into_shards from huggingface_hub library\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-10-16 13:37:56,133] [INFO] [engine.py:790:_run_pass] Pass awq:AutoAWQQuantizer finished in 662.193807 seconds\n",
            "[2024-10-16 13:37:56,134] [INFO] [cache.py:192:load_model] Loading model 13e1ac3a from cache.\n",
            "[2024-10-16 13:37:58,022] [INFO] [engine.py:435:run_no_search] Saved output model to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/awq/olive-cli-tmp-wlo5c24n/output_model\n",
            "[2024-10-16 13:37:58,022] [INFO] [engine.py:347:run_accelerator] Save footprint to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/awq/olive-cli-tmp-wlo5c24n/footprints.json.\n",
            "[2024-10-16 13:37:58,023] [INFO] [engine.py:274:run] Run history for cpu-cpu:\n",
            "[2024-10-16 13:37:58,023] [INFO] [engine.py:528:dump_run_history] Please install tabulate for better run history output\n",
            "Command succeeded. Output model saved to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/awq\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "olive quantize \\\n",
        "    --model_name_or_path microsoft/Phi-3.5-mini-instruct \\\n",
        "    --algorithm awq \\\n",
        "    --output_path models/phi/awq \\\n",
        "    --log_level 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxJCT5wioDfp"
      },
      "source": [
        "## üëü Train the model\n",
        "\n",
        "Next, the `olive finetune` command executes. \n",
        "\n",
        "üß† Olive supports the following models out-of-the-box: Phi, Llama, Mistral, Gemma, Qwen, Falcon and [many others](https://huggingface.co/docs/optimum/en/exporters/onnx/overview).\n",
        "\n",
        "‚òï It can take around 5-10mins for the finetuning complete. At the end of the process you will have an PEFT adapter.\n",
        "\n",
        "‚öôÔ∏è For more information on available options, read the [Olive Finetune documentation](https://microsoft.github.io/Olive/features/cli.html#finetune)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t36pRF2oDfq",
        "outputId": "8da643ba-bd7f-464d-8c90-47e6cfdbd364"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded previous command output of type hfmodel from models/phi/awq\n",
            "[2024-10-16 13:43:46,076] [INFO] [run.py:138:run_engine] Running workflow default_workflow\n",
            "[2024-10-16 13:43:46,089] [INFO] [cache.py:137:__init__] Using cache directory: /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/.olive-cache/default_workflow\n",
            "[2024-10-16 13:43:46,092] [INFO] [accelerator_creator.py:224:create_accelerators] Running workflow on accelerator specs: gpu-cuda\n",
            "[2024-10-16 13:43:46,100] [INFO] [engine.py:255:run] Running Olive on accelerator: gpu-cuda\n",
            "[2024-10-16 13:43:46,100] [INFO] [engine.py:897:_create_system] Creating target system ...\n",
            "[2024-10-16 13:43:46,100] [INFO] [engine.py:900:_create_system] Target system created in 0.000344 seconds\n",
            "[2024-10-16 13:43:46,100] [INFO] [engine.py:909:_create_system] Creating host system ...\n",
            "[2024-10-16 13:43:46,101] [INFO] [engine.py:912:_create_system] Host system created in 0.000137 seconds\n",
            "[2024-10-16 13:43:47,482] [INFO] [engine.py:718:_run_pass] Running pass f:LoRA {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-10-16 13:43:51,042] [INFO] [lora.py:546:train_and_save_new_model] Running fine-tuning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 106.4467, 'train_samples_per_second': 1.127, 'train_steps_per_second': 0.141, 'train_loss': 1.3030354817708334, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [01:46<00:00,  7.10s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-10-16 13:45:38,380] [INFO] [engine.py:790:_run_pass] Pass f:LoRA finished in 110.898584 seconds\n",
            "[2024-10-16 13:45:38,381] [INFO] [cache.py:192:load_model] Loading model 48230c00 from cache.\n",
            "[2024-10-16 13:45:40,548] [INFO] [engine.py:435:run_no_search] Saved output model to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft/olive-cli-tmp-8b4j0mwd/output_model\n",
            "[2024-10-16 13:45:40,549] [INFO] [engine.py:347:run_accelerator] Save footprint to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft/olive-cli-tmp-8b4j0mwd/footprints.json.\n",
            "[2024-10-16 13:45:40,549] [INFO] [engine.py:274:run] Run history for gpu-cuda:\n",
            "[2024-10-16 13:45:40,550] [INFO] [engine.py:528:dump_run_history] Please install tabulate for better run history output\n",
            "Command succeeded. Output model saved to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "olive finetune \\\n",
        "    --method lora \\\n",
        "    --model_name_or_path models/phi/awq \\\n",
        "    --trust_remote_code \\\n",
        "    --data_files \"data/data_sample_travel.jsonl\" \\\n",
        "    --data_name \"json\" \\\n",
        "    --text_template \"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\\n{response}<|end|>\" \\\n",
        "    --max_steps 15 \\\n",
        "    --output_path ./models/phi/ft \\\n",
        "    --log_level 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7woNXLDF0bhh"
      },
      "source": [
        "üìÇ The output is located in a folder named `models/phi/ft`. Below is a list of the folder - notice that OLIVE just produces the PEFT adapter (not the base model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-prKBy20U5m",
        "outputId": "295adab5-080d-4eff-93ba-dec092acf464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 145M\n",
            "drwxrwxr-x 2 azureuser azureuser 4.0K Oct 16 13:45 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxrwxr-x 4 azureuser azureuser 4.0K Oct 16 13:45 \u001b[01;34m..\u001b[0m/\n",
            "-rw-rw-r-- 1 azureuser azureuser 5.1K Oct 16 13:45 README.md\n",
            "-rw-rw-r-- 1 azureuser azureuser  744 Oct 16 13:45 adapter_config.json\n",
            "-rw-rw-r-- 1 azureuser azureuser 145M Oct 16 13:45 adapter_model.safetensors\n"
          ]
        }
      ],
      "source": [
        "%ls -lah models/phi/ft/adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîå Generate Adapters for ONNX Runtime\n",
        "\n",
        "Next, you need to generate the Hugging Face PEFT adapter into a format for the ONNX runtime. This command will:\n",
        "\n",
        "1. Convert the base model into ONNX format\n",
        "2. Optimize the base model for the ONNX runtime (e.g. graph optimization).\n",
        "3. Convert the adapter into an optimized format for the ONNX Runtime\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded previous command output of type hfmodel from models/phi/ft\n",
            "[2024-10-16 13:46:10,172] [INFO] [run.py:138:run_engine] Running workflow default_workflow\n",
            "[2024-10-16 13:46:10,185] [INFO] [cache.py:137:__init__] Using cache directory: /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/.olive-cache/default_workflow\n",
            "[2024-10-16 13:46:10,189] [INFO] [accelerator_creator.py:224:create_accelerators] Running workflow on accelerator specs: gpu-cuda\n",
            "[2024-10-16 13:46:10,192] [INFO] [engine.py:255:run] Running Olive on accelerator: gpu-cuda\n",
            "[2024-10-16 13:46:10,192] [INFO] [engine.py:897:_create_system] Creating target system ...\n",
            "[2024-10-16 13:46:10,192] [INFO] [engine.py:900:_create_system] Target system created in 0.000049 seconds\n",
            "[2024-10-16 13:46:10,192] [INFO] [engine.py:909:_create_system] Creating host system ...\n",
            "[2024-10-16 13:46:10,192] [INFO] [engine.py:912:_create_system] Host system created in 0.000040 seconds\n",
            "[2024-10-16 13:46:11,605] [INFO] [engine.py:718:_run_pass] Running pass c:OnnxConversion {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set your model on a GPU device in order to run your model.\n",
            "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "We suggest you to set `torch_dtype=torch.float16` for better efficiency with AWQ.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/cache_utils.py:447: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
            "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:98: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if sequence_length != 1:\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:261: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  seq_len = seq_len or torch.max(position_ids) + 1\n",
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:262: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if seq_len > self.original_max_position_embeddings:\n",
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:265: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  ext_factors = torch.tensor(self.short_factor, dtype=torch.float32, device=x.device)\n",
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/cache_utils.py:432: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
            "  elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n",
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/models/phi3/modeling_phi3.py:468: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
            "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
            "Could not locate the modeling_phi3.py inside /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft/model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-10-16 13:46:43,689] [WARNING] [utils.py:125:save_module_files] Failed to save module file for modeling_phi3.Phi3ForCausalLM: /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft/model does not appear to have a file named modeling_phi3.py. Checkout 'https://huggingface.co//home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft/model/tree/None' for available files.. Loading config with `trust_remote_code=True` will fail!\n",
            "[2024-10-16 13:46:43,800] [INFO] [engine.py:790:_run_pass] Pass c:OnnxConversion finished in 32.195229 seconds\n",
            "[2024-10-16 13:46:43,802] [INFO] [engine.py:718:_run_pass] Running pass o:OrtTransformersOptimization {}\n",
            "[2024-10-16 13:47:25,679] [INFO] [engine.py:790:_run_pass] Pass o:OrtTransformersOptimization finished in 41.876406 seconds\n",
            "[2024-10-16 13:47:25,681] [INFO] [engine.py:718:_run_pass] Running pass e:ExtractAdapters {}\n",
            "[2024-10-16 13:47:31,369] [INFO] [engine.py:790:_run_pass] Pass e:ExtractAdapters finished in 5.688837 seconds\n",
            "[2024-10-16 13:47:31,376] [INFO] [engine.py:718:_run_pass] Running pass m:ModelBuilder {}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:991: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GroupQueryAttention (GQA) is used in this model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:985: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving GenAI config in /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/.olive-cache/default_workflow/runs/04ec9341/models\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/azureuser/miniconda3/envs/olive-jambay/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving processing files in /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/.olive-cache/default_workflow/runs/04ec9341/models for GenAI\n",
            "[2024-10-16 13:47:31,592] [INFO] [engine.py:790:_run_pass] Pass m:ModelBuilder finished in 0.216728 seconds\n",
            "[2024-10-16 13:47:31,600] [INFO] [cache.py:192:load_model] Loading model 04ec9341 from cache.\n",
            "[2024-10-16 13:47:33,578] [INFO] [engine.py:435:run_no_search] Saved output model to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft-onnx/olive-cli-tmp-9z23lgcb/output_model\n",
            "[2024-10-16 13:47:33,581] [INFO] [engine.py:347:run_accelerator] Save footprint to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft-onnx/olive-cli-tmp-9z23lgcb/footprints.json.\n",
            "[2024-10-16 13:47:33,585] [INFO] [engine.py:274:run] Run history for gpu-cuda:\n",
            "[2024-10-16 13:47:33,585] [INFO] [engine.py:528:dump_run_history] Please install tabulate for better run history output\n",
            "Command succeeded. Output model saved to /home/azureuser/code/Ignite_FineTuning_workshop/lab/workshop-instructions/lab5-optimize-model/models/phi/ft-onnx\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "olive generate-adapter \\\n",
        "    --model_name_or_path models/phi/ft \\\n",
        "    --use_ort_genai \\\n",
        "    --output_path models/phi/ft-onnx \\\n",
        "    --log_level 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Uwm432loDfr"
      },
      "source": [
        "## üß™ Quick test\n",
        "\n",
        "The code below creates a test app that consumes the model in a simple console chat interface. You will be prompted to enter an input. Here are some phrases to try:\n",
        "\n",
        "- \"Cricket is a great game\"\n",
        "- \"I was taken aback by the size of the whale\"\n",
        "- \"there was concern about the dark lighting on the street\"\n",
        "\n",
        "üßë‚Äçüíª Below we show the Python API for the ONNX Runtime. However, other language bindings are available in [Java, C#, C++](https://github.com/microsoft/onnxruntime-genai/tree/main/examples).\n",
        "\n",
        "üö™To exit the chat interface, enter `exit` or select `Ctrl+c`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puMdoAxjoDfr",
        "outputId": "b652c88f-5353-4f26-d8bd-83bd79a2ece5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter phrase: cricket is an amazing game!\n",
            "Output: \n",
            "joy\n",
            "Input: exit\n"
          ]
        }
      ],
      "source": [
        "import onnxruntime_genai as og\n",
        "import numpy as np\n",
        "from olive.common.utils import load_weights\n",
        "import os\n",
        "\n",
        "model_folder = \"models/phi/ft-onnx/model\"\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "model = og.Model(model_folder)\n",
        "tokenizer = og.Tokenizer(model)\n",
        "tokenizer_stream = tokenizer.create_stream()\n",
        "\n",
        "# Load the LoRA adapter weights\n",
        "weights_file = os.path.join(model_folder, \"adapter_weights.onnx_adapter\")\n",
        "\n",
        "adapters = {\n",
        "    \"travel\": {\n",
        "        \"weights\": weights_file,\n",
        "        \"template\": \"<|user|>\\n{input}</s>\\n<|assistant|>\"\n",
        "    }\n",
        "}\n",
        "\n",
        "adapters_weights = {\n",
        "    key: load_weights(value[\"weights\"]) for key, value in adapters.items()\n",
        "}\n",
        "\n",
        "# Set the max length to something sensible by default,\n",
        "# since otherwise it will be set to the entire context length\n",
        "search_options = {}\n",
        "search_options['max_length'] = 200\n",
        "search_options['past_present_share_buffer'] = False\n",
        "\n",
        "chat_template = \"<|user|>\\n{input}</s>\\n<|assistant|>\"\n",
        "\n",
        "text = input(\"Input: \")\n",
        "\n",
        "# Keep asking for input phrases\n",
        "while text != \"exit\":\n",
        "  if not text:\n",
        "    print(\"Error, input cannot be empty\")\n",
        "    exit\n",
        "\n",
        "  # generate prompt (prompt template + input)\n",
        "  prompt = f'{chat_template.format(input=text)}'\n",
        "\n",
        "  # encode the prompt using the tokenizer\n",
        "  input_tokens = tokenizer.encode(prompt)\n",
        "\n",
        "  # the adapter weights are added to the model at inference time. This means you\n",
        "  # can select different adapters for different tasks i.e. multi-LoRA.\n",
        "\n",
        "  params = og.GeneratorParams(model)\n",
        "  for k, v in adapter_weights.items():\n",
        "    params.set_model_input(k, v)\n",
        "  params.set_search_options(**search_options)\n",
        "  params.input_ids = input_tokens\n",
        "  generator = og.Generator(model, params)\n",
        "\n",
        "  print(\"Output: \", end='', flush=True)\n",
        "  # stream the output\n",
        "  try:\n",
        "    while not generator.is_done():\n",
        "      generator.compute_logits()\n",
        "      generator.generate_next_token()\n",
        "\n",
        "      new_token = generator.get_next_tokens()[0]\n",
        "      print(tokenizer_stream.decode(new_token), end='', flush=True)\n",
        "  except KeyboardInterrupt:\n",
        "      print(\"  --control+c pressed, aborting generation--\")\n",
        "\n",
        "  print()\n",
        "  text = input(\"Input: \")\n",
        "\n",
        "# delete the objects to free up resources.\n",
        "del generator\n",
        "del model\n",
        "del tokenizer\n",
        "del tokenizer_stream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCWbVFvuAtAl"
      },
      "source": [
        "## Publish to Hugging Face\n",
        "\n",
        "ü§ó You'll need to get a token from https://huggingface.co/settings/tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph3NXYRZA0fO",
        "outputId": "d22116bf-9c8a-435b-ed10-93bbd2cf0466"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Cell magic `%%shell` not found.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "# update these parameters\n",
        "TOKEN=\"\" # get a token from https://huggingface.co/settings/tokens\n",
        "REPO_ID=\"\" # for example username/repo-name\n",
        "MODEL_PATH=\"models/phi/ft-onnx\" # no need to change\n",
        "\n",
        "huggingface-cli upload --token $TOKEN $REPO_ID $MODEL_PATH"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "olive-jambay",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "077ebf9f5be54cc083697f77698e5ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c18ad862d994fbfa7026b7b83925c2c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_091861fe24704925ab3f7a30bd1179d5",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
          }
        },
        "086dc4c982db41a0bc8c0131b79e50e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "091861fe24704925ab3f7a30bd1179d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12c5ab0e35424b7b97b64802f425a5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "138535bf368b48ca9d5bc1f007f843ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bd650f5bf5433db248772ad5bfe3d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_590d57c9b3f14250b48d3422b60992a0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e4eaa3c7776343af9f54308b01db55f4",
            "value": "Downloading‚Äáreadme:‚Äá100%"
          }
        },
        "344cf705b19b4549beed33873ccb2876": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc93b5f2f32741a4af2a4f5a2c4a9e9c",
              "IPY_MODEL_93d817aded5748ae93aee70d35c9d2a2",
              "IPY_MODEL_45a0e8351ceb40339cd607d39d04d7a1"
            ],
            "layout": "IPY_MODEL_e5cef26102904223a1432ac645db3dfe"
          }
        },
        "3c18ad862d994fbfa7026b7b83925c2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c87e2e2e92545809c6f8e6168dcc36c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45a0e8351ceb40339cd607d39d04d7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d62297cd89d04114b76e57a3183fdfbd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_942a390409a648d8b980d5e7c62ee900",
            "value": "‚Äá188k/188k‚Äá[00:00&lt;00:00,‚Äá275kB/s]"
          }
        },
        "4ae2eaf6c01e45038e77a2bf1ab22a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "590d57c9b3f14250b48d3422b60992a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d8eaec8076a411198b291859ab1c9b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_138535bf368b48ca9d5bc1f007f843ac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e65680b8f29d491186d075b59c9e3d79",
            "value": "‚Äá24.0/24.0‚Äá[00:00&lt;00:00,‚Äá95.3B/s]"
          }
        },
        "680edc52159147eabaee8d0ad171f879": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "697c911fc7224cd88364293abe822d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc0de3b5c3624715b4421ea6c1f858cf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_086dc4c982db41a0bc8c0131b79e50e7",
            "value": "‚Äá1464/1464‚Äá[00:00&lt;00:00,‚Äá19436.39‚Äáexamples/s]"
          }
        },
        "7c261f5347b540e4b357348ff0e873c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aef653a8665451c878f32d13c0bf301": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9117830d46384e9a9c3c98c1b0d5bd8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93d817aded5748ae93aee70d35c9d2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4def9b16cd34f36bb4974757cce1319",
            "max": 187885,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8aef653a8665451c878f32d13c0bf301",
            "value": 187885
          }
        },
        "942a390409a648d8b980d5e7c62ee900": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4def9b16cd34f36bb4974757cce1319": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4b65045cd8c4474a9dd875d3b3d4e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_077ebf9f5be54cc083697f77698e5ba6",
              "IPY_MODEL_ee2aecdb35244cc0847a5f1363502018",
              "IPY_MODEL_697c911fc7224cd88364293abe822d02"
            ],
            "layout": "IPY_MODEL_4ae2eaf6c01e45038e77a2bf1ab22a4e"
          }
        },
        "d62297cd89d04114b76e57a3183fdfbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc0de3b5c3624715b4421ea6c1f858cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb0a1f3357b4e6090cce383a0368aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27bd650f5bf5433db248772ad5bfe3d9",
              "IPY_MODEL_f5272b5ed355411db8baeb2b360cea5f",
              "IPY_MODEL_5d8eaec8076a411198b291859ab1c9b6"
            ],
            "layout": "IPY_MODEL_f42d39f701de48888145efa92da5fa02"
          }
        },
        "e3389975c0f443ad90d6f8cee4710577": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4eaa3c7776343af9f54308b01db55f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5cef26102904223a1432ac645db3dfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e65680b8f29d491186d075b59c9e3d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee2aecdb35244cc0847a5f1363502018": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c261f5347b540e4b357348ff0e873c5",
            "max": 1464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12c5ab0e35424b7b97b64802f425a5b9",
            "value": 1464
          }
        },
        "f42d39f701de48888145efa92da5fa02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5272b5ed355411db8baeb2b360cea5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9117830d46384e9a9c3c98c1b0d5bd8c",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c87e2e2e92545809c6f8e6168dcc36c",
            "value": 24
          }
        },
        "fc93b5f2f32741a4af2a4f5a2c4a9e9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3389975c0f443ad90d6f8cee4710577",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_680edc52159147eabaee8d0ad171f879",
            "value": "Downloading‚Äádata:‚Äá100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
